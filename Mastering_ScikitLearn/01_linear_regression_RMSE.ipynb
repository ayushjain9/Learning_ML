{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "\n",
    "- What are some **evaluation metrics** for regression problems?\n",
    "- How do I choose **which features to include** in my model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of supervised learning\n",
    "\n",
    "- **Classification:** Predict a categorical response\n",
    "- **Regression:** Predict a continuous response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Form of linear regression\n",
    "\n",
    "$y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_nx_n$\n",
    "\n",
    "- $y$ is the response\n",
    "- $\\beta_0$ is the intercept\n",
    "- $\\beta_1$ is the coefficient for $x_1$ (the first feature)\n",
    "- $\\beta_n$ is the coefficient for $x_n$ (the nth feature)\n",
    "\n",
    "In this case:\n",
    "\n",
    "$y = \\beta_0 + \\beta_1 \\times TV + \\beta_2 \\times Radio + \\beta_3 \\times Newspaper$\n",
    "\n",
    "The $\\beta$ values are called the **model coefficients**. These values are \"learned\" during the model fitting step using the \"least squares\" criterion. Then, the fitted model can be used to make predictions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression in scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# instantiate\n",
    "linreg = LinearRegression()\n",
    "\n",
    "# fit the model to the training data (learn the coefficients)\n",
    "linreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting model coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8769666223179318\n",
      "[0.04656457 0.17915812 0.00345046]\n"
     ]
    }
   ],
   "source": [
    "# print the intercept and coefficients\n",
    "print(linreg.intercept_)\n",
    "print(linreg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('TV', 0.04656456787415029),\n",
       " ('Radio', 0.17915812245088839),\n",
       " ('Newspaper', 0.003450464711180378)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pair the feature names with the coefficients\n",
    "list(zip(feature_cols, linreg.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$y = 2.88 + 0.0466 \\times TV + 0.179 \\times Radio + 0.00345 \\times Newspaper$$\n",
    "\n",
    "How do we interpret the **TV coefficient** (0.0466)?\n",
    "\n",
    "- For a given amount of Radio and Newspaper ad spending, **a \"unit\" increase in TV ad spending** is associated with a **0.0466 \"unit\" increase in Sales**.\n",
    "- Or more clearly: For a given amount of Radio and Newspaper ad spending, **an additional $1,000 spent on TV ads** is associated with an **increase in sales of 46.6 items**.\n",
    "\n",
    "Important notes:\n",
    "\n",
    "- This is a statement of **association**, not **causation**.\n",
    "- If an increase in TV ad spending was associated with a **decrease** in sales, $\\beta_1$ would be **negative**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on the testing set\n",
    "y_pred = linreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need an **evaluation metric** in order to compare our predictions with the actual values!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation metrics for regression\n",
    "\n",
    "Evaluation metrics for classification problems, such as **accuracy**, are not useful for regression problems. Instead, we need evaluation metrics designed for comparing continuous values.\n",
    "\n",
    "Let's create some example numeric predictions, and calculate **three common evaluation metrics** for regression problems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define true and predicted response values\n",
    "true = [100, 50, 30, 20]\n",
    "pred = [90, 50, 50, 30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mean Absolute Error** (MAE) is the mean of the absolute value of the errors:\n",
    "\n",
    "$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n",
      "10.0\n"
     ]
    }
   ],
   "source": [
    "# calculate MAE by hand\n",
    "print((10 + 0 + 20 + 10)/4.)\n",
    "\n",
    "# calculate MAE using scikit-learn\n",
    "from sklearn import metrics\n",
    "print(metrics.mean_absolute_error(true, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mean Squared Error** (MSE) is the mean of the squared errors:\n",
    "\n",
    "$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150.0\n",
      "150.0\n"
     ]
    }
   ],
   "source": [
    "# calculate MSE by hand\n",
    "print((10**2 + 0**2 + 20**2 + 10**2)/4.)\n",
    "\n",
    "# calculate MSE using scikit-learn\n",
    "print(metrics.mean_squared_error(true, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Root Mean Squared Error** (RMSE) is the square root of the mean of the squared errors:\n",
    "\n",
    "$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.24744871391589\n",
      "12.24744871391589\n"
     ]
    }
   ],
   "source": [
    "# calculate RMSE by hand\n",
    "import numpy as np\n",
    "print(np.sqrt((10**2 + 0**2 + 20**2 + 10**2)/4.))\n",
    "\n",
    "# calculate RMSE using scikit-learn\n",
    "print(np.sqrt(metrics.mean_squared_error(true, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing these metrics:\n",
    "\n",
    "- **MAE** is the easiest , because it's the average error.\n",
    "- **MSE** is more popular than MAE, because MSE \"punishes\" larger errors.\n",
    "- **RMSE** is even more popular than MSE, because RMSE is interpretable in the \"y\" units."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
